import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.neighbors import KNeighborsClassifier
from sklearn.metrics import confusion_matrix, f1_score, classification_report
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns

METRICS = ['euclidean', 'manhattan', 'cosine']


def load_data(gender: str = "all"):
    formant_labels = ['Formant 1', 'Formant 2', 'Formant 3']
    data = pd.read_csv('data.csv').dropna(subset=formant_labels)
    if gender in ["F", "M"]:
        data = data[data['Gender'] == gender]
    X = data[formant_labels].values
    y = data['Vowel Phoneme'].values
    return X, y


def transform_matrix_string(matrix: str | np.ndarray, table_format: bool = False):
    formatted_rows = []

    if isinstance(matrix, str):
        rows = matrix.strip().split('\n')
        for row in rows:
            clean_row = row.strip().replace('[', '').replace(']', '')
            elements = clean_row.split()
            formatted_row = ' & '.join(elements)
            formatted_rows.append(formatted_row)
    elif isinstance(matrix, np.ndarray):
        for row in matrix:
            formatted_row = ' & '.join(map(str, row))
            formatted_rows.append(formatted_row)
    else:
        raise ValueError("Matrix must be a string or a numpy array")

    if table_format:
        latex_matrix = ' \\\\ '.join(formatted_rows)
        latex_string = (
            "\\("
            "    \\begin{bmatrix}"
            f"        {latex_matrix}"
            "    \\end{bmatrix}"
            "\\)"
        )
    else:
        latex_matrix = ' \\\\\n'.join(formatted_rows)

        latex_string = (
            "\\(\n"
            "    \\begin{bmatrix}\n"
            f"        {latex_matrix}\n"
            "    \\end{bmatrix}\n"
            "\\)"
        )

    return latex_string


def plot_confusion_matrix(conf_matrix, labels):
    plt.figure(figsize=(10, 7))
    sns.heatmap(conf_matrix, annot=True, fmt='d', cmap='Blues', xticklabels=labels, yticklabels=labels)
    plt.ylabel('Actual')
    plt.xlabel('Predicted')
    plt.title('Confusion Matrix')
    plt.show()


def train_evaluate_knn(X, y, k: int = 5, test_size: float = 0.25, random_state: int = None, metric: str = 'euclidean'):
    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=test_size, random_state=random_state)

    knn = KNeighborsClassifier(n_neighbors=k, metric=metric)
    knn.fit(X_train, y_train)

    y_pred = knn.predict(X_test)

    conf_matrix = confusion_matrix(y_test, y_pred)
    f1 = f1_score(y_test, y_pred, average='micro')

    return conf_matrix, f1


def print_multiple_knn_results(n_runs: int = 7, n_k: int = 7, metrics: list[str] = METRICS):
    X, y = load_data()

    for metric in metrics:
        print(f"### {metric.capitalize()} Distance", end="")
        print("\n")

        print("| K |", end="")
        for i in range(1, n_runs + 1):
            print(f" F1 score \\ Run {i} |", end="")
        print(" F1 Score Average |")
        print("|---|" + "---|" * n_runs + "---|")

        for k in range(2, n_k+1):  # You can adjust the range of K values as needed
            print(f"| {k} |", end="")
            f1_scores = []
            for i in range(1, n_runs + 1):
                conf_matrix, f1 = train_evaluate_knn(X, y, k=k, random_state=i, metric=metric)
                f1_scores.append(f1)
                print(f" {f1:.5f} |", end="")
        average_f1_score = np.mean(f1_scores)
        print(f" {average_f1_score:.5f} |")
        print("\n")


def print_knn_gender_results(n_runs: int = 7, k: int = 5, metrics: list[str] = METRICS):
    for metric in metrics:
        print(f"### {metric.capitalize()} Distance", end="")
        print("\n")

        print("| Gender |", end="")
        for i in range(1, n_runs + 1):
            print(f" F1 score \\ Run {i} |", end="")
        print(" F1 Score Average |")
        print("|---|" + "---|" * n_runs + "---|")

        for gender in ["F", "M", "all"]:
            print(f"| {gender} |", end="")
            X, y = load_data(gender=gender)
            f1_scores = []
            for i in range(1, n_runs + 1):
                conf_matrix, f1 = train_evaluate_knn(X, y, k=k, random_state=i, metric=metric)
                f1_scores.append(f1)
                print(f" {f1:.5f} |", end="")
            average_f1_score = np.mean(f1_scores)
            print(f" {average_f1_score:.5f} |")
        print("\n")


def print_knn_results(n_runs: int = 7, k: int = 5, metrics: list[str] = METRICS):
    X, y = load_data(gender="all")
    labels = np.unique(y)
    for metric in metrics:
        print(f"### {metric.capitalize()} Distance", end="")
        print("\n")

        print("| Run | Confusion matrix | F1 Score |")
        print("|---|---|---|")

        f1_scores = []
        n_run = 7
        for i in range(1, n_run+1):
            print(f"| {i} |", end="")
            conf_matrix, f1 = train_evaluate_knn(X, y, k=k, random_state=i, metric=metric)
            f1_scores.append(f1)
            plot_confusion_matrix(conf_matrix, labels)
            print(f" | {f1:.5f} |")
        print("\n")
        print(f"Average F1 score: {np.mean(f1_scores)}")
        print("\n")


# print_multiple_knn_results(n_runs=7, n_k=7)
# print_knn_gender_results(n_runs=7, k=5)
print_knn_results(n_runs=7, k=5)
